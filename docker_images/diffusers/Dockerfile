FROM tiangolo/uvicorn-gunicorn:python3.8
LABEL maintainer="Nicolas Patry <nicolas@huggingface.co>"

# Add any system dependency here
# RUN apt-get update -y && apt-get install libXXX -y

RUN pip install --no-cache-dir torch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0
COPY ./requirements.txt /app
RUN pip install --no-cache-dir -r requirements.txt
# Specifically built for T4.
RUN pip install https://huggingface.co/datasets/Narsil/test/resolve/main/xformers-0.0.14.dev0-cp38-cp38-linux_x86_64.whl
COPY ./prestart.sh /app/


# Most DL models are quite large in terms of memory, using workers is a HUGE
# slowdown because of the fork and GIL with python.
# Using multiple pods seems like a better default strategy.
# Feel free to override if it does not make sense for your library.
ARG max_workers=1
ENV MAX_WORKERS=$max_workers
ENV HUGGINGFACE_HUB_CACHE=/data
ENV DIFFUSERS_CACHE=/data
ENV HF_HOME=/data

# Necessary on GPU environment docker.
# TIMEOUT env variable is used by nvcr.io/nvidia/pytorch:xx for another purpose
# rendering TIMEOUT defined by uvicorn impossible to use correctly
# We're overriding it to be renamed UVICORN_TIMEOUT
# UVICORN_TIMEOUT is a useful variable for very large models that take more
# than 30s (the default) to load in memory.
# If UVICORN_TIMEOUT is too low, uvicorn will simply never loads as it will
# kill workers all the time before they finish.
RUN sed -i 's/TIMEOUT/UVICORN_TIMEOUT/g' /gunicorn_conf.py
COPY ./app /app/app
